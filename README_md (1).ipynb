{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RNU4NySFDxxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Mobile Price Range Prediction**"
      ],
      "metadata": {
        "id": "B9bdBmpRDHUy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest Classification**"
      ],
      "metadata": {
        "id": "qTSGFJcGDbgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest classifier creates a set of decision trees from randomly selected subset of training set. It then aggregates the votes from different decision trees to decide the final class of the test object.Ensembled algorithms are those which combines more than one algorithms of same or different kind for classifying objects. For example, running prediction over Naive Bayes, SVM and Decision Tree and then taking vote for final consideration of class for test object.Basic parameters to Random Forest Classifier can be total number of trees to be generated and decision tree related parameters like minimum split, split criteria etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "z0b3P7XqDbm_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **XGboost**"
      ],
      "metadata": {
        "id": "tMQ0imyJDbxX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost stands for “Extreme Gradient Boosting”. XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements Machine Learning algorithms under the Gradient Boosting framework. It provides a parallel tree boosting to solve many data science problems in a fast and accurate way.\n",
        "\n"
      ],
      "metadata": {
        "id": "uEdvWdVaEGhl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Execution Instruction**"
      ],
      "metadata": {
        "id": "dORWTnvJEGkJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This file must be executed, to define all the functions and variables required for classification operations which leads to the production of the model.h5 file. and to evaluate the model performance on unseen data"
      ],
      "metadata": {
        "id": "pyBvmourDb1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion**"
      ],
      "metadata": {
        "id": "pl2_MZetFOj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1- From EDA we can see that here are mobile phones in 4 price ranges. The number of elements is almost similar. half the devices have Bluetooth, and half don’t\n",
        "\n",
        "2- There is a gradual increase in battery as the price range increases\n",
        "\n",
        "3- Ram has continuous increase with price range while moving from Low cost to Very high cost\n",
        "\n",
        "4- Costly phones are lighter\n",
        "\n",
        "5- RAM, battery power, pixels played more significant role in deciding the price range of mobile phone.\n",
        "\n",
        "6- form all the above experiments we can conclude that logistic regression and, XGboosting with using hyperparameters we got the best results"
      ],
      "metadata": {
        "id": "xIuNnAZcFRtn"
      }
    }
  ]
}